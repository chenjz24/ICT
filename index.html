<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="<b>ICT</b>: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Hallucination mitigation, LVLMs, Inference intervention">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ICT</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"> ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=KUXvSuIAAAAJ&hl=zh-CN" target="_blank"><font color="#B082C9"><b>Junzhe Chen</b></font></a><sup>1,2*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=djUbDjQAAAAJ&hl=en" target="_blank"><font color="#B082C9"><b>Tianshu Zhang</b></font></a><sup>1*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="http://tartrl.cn/hsy" target="_blank"><font color="#B082C9"><b>Shiyu Huang</b></font></a><sup>3</sup>&emsp;
                </span> 
                <span class="author-block">
                  <a href="https://github.com/purshow" target="_blank"><font color="#B082C9"><b>Yuwei Niu</b></font></a><sup>4</sup>&emsp;
                </span>
              <br>
                <span class="author-block">
                  <a href="http://www.zhanglinfeng.tech/" target="_blank"><font color="#B082C9"><b>Linfeng Zhang</b></font></a><sup>5</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.thss.tsinghua.edu.cn/en/faculty/lijiewen.htm" target="_blank"><font color="#B082C9"><b>Lijie Wen
                  </b></font></a><sup>1†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://xuminghu.github.io/" target="_blank"><font color="#B082C9"><b>Xuming Hu</b></font></a><sup>2†</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Tsinghua University&emsp;
                      <sup>2</sup>HKUST(GZ)&emsp;
                      <sup>3</sup>Zhipu AI
                      <br>
                      <sup>4</sup>Chongqing University&emsp;
                      <sup>5</sup>Shanghai Jiao Tong University
                    </span> <br>
                    <span class="author-block">
                      <h1 class="title is-4"><font color="#B03A2E"><b>CVPR 2025</b></font></h1>
                    </span>
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2411.15268" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  
                  <span class="link-block">
                    <a href="https://github.com/THU-BPM/ICT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Comparison with Contrastive Decoding</h2>
        <div class="content has-text-justified" >
          <img src="static/images/overview_00.png" width="70%" style="display: block; margin: auto;" />
            <br>
          <p>
            Unlike contrastive decoding, ICT <b>does not eliminate language priors</b> to reduce the model’s over-reliance on text modality. Instead, it intervenes during the <b>forward pass</b> to enhance the model’s focus on both comprehensive visual information and fine-grained object details.
            As illustrated in the Figure, after applying ICT, the model is able to focus more on the details within the image, such as identifying the man as Curry, while simultaneously utilizing beneficial language priors (e.g., Curry is a basketball player) to infer and arrive at the correct answer. Since the intervention shift vectors are pre-computed, ICT <b>does not introduce additional delays</b> during the forward pass.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">ICT: Image-Object Cross-Level Trusted Intervention</h2>
        <div class="content has-text-justified">
            <img src="static/images/pipeline.png" height="100%"/>
            <br>
          <p>
            We propose ICT, a novel, training-free, plug-and-play method that effectively reduces hallucinations in LVLMs by enhancing focus on both overall visual information and fine-grained object details during the forward pass, without eliminating beneficial language priors.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/casestudy_00.png" width="100%"/>
        <br>
        <h2 class="content has-text-justified">
          <b>Top</b>: After applying ICT, the model allocates a higher proportion of attention to visual tokens, especially to object tokens relevant to the question (e.g., ``horse'' and ``fruits''). By prioritizing visual information, ICT correctly identifies the absence of a horse in the image, whereas VCD erroneously concludes that a horse is present due to insufficient attention to visual cues. 
          <br>
          <b>Bottom</b>:When asked, "How many uncut fruits are in the picture?", VCD incorrectly answered ``two'' due to a lack of focus on visual details. Although ICT correctly identifies that there are a total of four fruits in the image, this question requires not only attention to visual content but also reasoning within the text modality. The model needs to not only recognize the overall count of fruits but also focus on the attribute ``uncut''. Due to its failure to incorporate this information, ICT outputs a wrong answer.
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Quantitative -->
<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Quantitative Results</h2>
        <img src="static/images/result.png" height="90%"/>
        <br>
        <h2 class="content has-text-justified">
          Table 1 presents the results of LLaVA-v1.5 and Qwen-VL on nine POPE dataset subsets, leading to the following conclusions:
          <br>
          <b>ICT Improves Performance:</b> Applying ICT boosts the F1 score by 7.09% for LLaVA-v1.5 and 5.44% for Qwen-VL, surpassing the previous SOTA baseline (Opera) by 2.19% and 1.14%, respectively. This improvement stems from ICT enhancing attention to visual information without eliminating useful language priors, reducing hallucinations.
          <br>
          <b>Multi-Level Interventions Help:</b> Image-level and object-level interventions achieve average F1 gains of 5.76% and 5.47%, respectively, showing that enhancing visual attention at different levels effectively mitigates hallucinations. Object-level intervention slightly outperforms as it implicitly broadens the model’s focus.
          <br>

          <b>ICT Generalizes Well:</b> An intervention shift vector trained on 1,500 MSCOCO samples improves the F1 score by 7.67% on MSCOCO and 6.09% across other subsets. This suggests ICT captures a general trustworthiness direction rather than overfitting specific datasets.
        </h2>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>
        @article{chen2024ict,
          title={ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models},
          author={Chen, Junzhe and Zhang, Tianshu and Huang, Shiyu and Niu, Yuwei and Zhang, Linfeng and Wen, Lijie and Hu, Xuming},
          journal={arXiv preprint arXiv:2411.15268},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
